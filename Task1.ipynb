# Install PySpark
!pip install -q pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count
import pandas as pd, numpy as np, shutil
from IPython.display import FileLink

# Spark session
spark = SparkSession.builder.appName("BigDataProcessing").getOrCreate()

# Generate and save synthetic CSV
pd.DataFrame({
    "id": np.arange(1, 1_000_001),
    "age": np.random.randint(18, 70, 1_000_000),
    "salary": np.random.randint(30000, 150000, 1_000_000),
    "department": np.random.choice(["HR", "Engineering", "Sales", "Marketing"], 1_000_000)
}).to_csv("synthetic_data.csv", index=False)

# Load data
data = spark.read.csv("synthetic_data.csv", header=True, inferSchema=True)

# Print schema
print("Schema:")
data.printSchema()

# Processing
high_earners = data.filter(col("salary") > 100000)
avg_salary = data.groupBy("department").agg(avg("salary").alias("avg_salary"))
age_group_count = data.withColumn("age_group", (col("age") / 10).cast("int") * 10) \
                      .groupBy("age_group").agg(count("*").alias("count"))

# Show insights
print("Top 5 High Earners:"); high_earners.show(5)
print("Avg Salary by Dept:"); avg_salary.show()
print("Employee Count by Age Group:"); age_group_count.orderBy("age_group").show()

# Save and zip output
avg_salary.coalesce(1).write.option("header", "true").csv("avg_salary_output", mode="overwrite")
shutil.make_archive('avg_salary_output', 'zip', 'avg_salary_output')

# Download link
FileLink("avg_salary_output.zip")
